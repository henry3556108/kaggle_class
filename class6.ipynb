{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "867f0b1d",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "Naive Bayes 是一種基於貝葉斯定理的分類技術，非常適合用於大量數據集。它假設所有的特徵都是相互獨立的，這意味著一個特徵或屬性在類別中的存在與其他特徵無關。雖然這是一個很強的假設，但在實際應用中，Naive Bayes 分類器表現出來的結果往往令人意外地好。\n",
    "\n",
    "#### 貝葉斯定理\n",
    "貝葉斯定理是這種分類器的核心，公式如下：\n",
    "$P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)} $\n",
    "其中：\n",
    "- $ P(A|B) $ 是在已知 B 發生後 A 的條件概率。\n",
    "- $ P(B|A) $ 是在已知 A 發生後 B 的條件概率。\n",
    "- $ P(A) $ 和 $ P(B) $ 是 A 和 B 的邊際概率。\n",
    "\n",
    "\n",
    "#### 優點\n",
    "1. 簡單而高效：Naive Bayes 模型由於其簡單性，在訓練和預測時都非常快速。\n",
    "\n",
    "2. 需要的訓練數據較少：與更複雜的分類方法相比，Naive Bayes 需要較少的訓練數據來估計必要的參數。\n",
    "\n",
    "#### 缺點\n",
    "1. 特徵獨立性的假設通常不實際：Naive Bayes 的主要限制是它假設所有的特徵在給定類別的條件下是相互獨立的，這在實際應用中往往不成立。\n",
    "\n",
    "2. 對於數值特徵的處理可能不是很好：Naive Bayes 在處理包含連續數據的特徵時表現不如一些其他算法。\n",
    "\n",
    "- [延伸閱讀](https://roger010620.medium.com/%E8%B2%9D%E6%B0%8F%E5%88%86%E9%A1%9E%E5%99%A8-naive-bayes-classifier-%E5%90%ABpython%E5%AF%A6%E4%BD%9C-66701688db02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6599972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Multinomial Naive Bayes: 0.64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 加載數據集\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 分割數據集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# 創建 Multinomial Naive Bayes 分類器\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "# 訓練模型\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred = mnb.predict(X_test)\n",
    "\n",
    "# 計算準確率\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of Multinomial Naive Bayes: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c238a604",
   "metadata": {},
   "source": [
    "- [官方文件](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes)\n",
    "- [statQuest Naive Bayes](https://www.youtube.com/watch?v=O2L2Uv9pdDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f3f185",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVM)\n",
    "SVM 是一種強大的分類方法，用於在數據中找到最佳分割線或決策邊界，稱為「最大邊際超平面」。它可以有效地進行線性和非線性分類。\n",
    "\n",
    "#### 工作原理\n",
    "- **線性 SVM**：在數據點之間找到最大的邊際。這個「邊際」是指任何類別中最近點到超平面的距離，這個距離應該是最大化的。\n",
    "- **核技巧**：當數據不是線性可分時，SVM 會使用所謂的「核技巧」來將數據映射到更高維度的空間，在這個空間中，數據可能是線性可分的。\n",
    "\n",
    "#### SVM 核\n",
    "常用的核函數包括：\n",
    "- **線性核**：用於線性可分的數據。\n",
    "- **多項式核**：用於非線性可分的數據。\n",
    "- **徑向基核（RBF）**：也稱高斯核，這是一種非常強大的核，特別適用於處理非線性分割。\n",
    "\n",
    "#### 優點\n",
    "1. 有效於高維空間：SVM 在高維空間中效果良好，甚至在特徵數量超過樣本數量的情況下仍然有效。\n",
    "2. 在非線性分類問題中表現出色：通過使用不同的核函數，SVM 可以有效地處理非線性邊界問題。\n",
    "3. 泛化錯誤率低：SVM 的優化目標是結構風險最小化，從理論上來說，這有助於提高模型的泛化能力。\n",
    "#### 缺點\n",
    "1. 選擇合適的核函數不易：核的選擇是 SVM 模型成功的關鍵，但沒有一個通用規則來指導如何選擇最佳核函數。\n",
    "2. 對大規模數據集計算量大：對於大型數據集，SVM 的訓練時間可能會非常長，這使其難以應用於大規模數據。\n",
    "3. 對參數和核函數的敏感性：SVM 的性能高度依賴於設定的參數（如 C 參數和核參數），這可能需要通過交叉驗證等方法進行仔細的調整。\n",
    "\n",
    "- [StatQuest svm-1](https://www.youtube.com/watch?v=efR1C6CvhmE)\n",
    "\n",
    "- [延伸閱讀](https://docs.pingcode.com/ask/47003.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64899a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# 創建 SVM 分類器\n",
    "svm = SVC(kernel='rbf', random_state=0, gamma=.10, C=1.0)\n",
    "\n",
    "# 訓練模型\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# 計算準確率\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of SVM with RBF kernel: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2355ffe",
   "metadata": {},
   "source": [
    "#### 參數解釋\n",
    "`kernel='linear'`: 選擇核函數為線性，這是最簡單的 SVM，用於線性可分的數據。\n",
    "\n",
    "`C=1.0`: 正則化參數。C 的值越大，分類器將嘗試最大化正確分類的數量，即使這可能導致較大的總體錯誤（過擬合的風]險增加）。\n",
    "\n",
    "`random_state=1`: 確保 SVM 的隨機過程（如數據洗牌等）的可重現性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312b4e9c",
   "metadata": {},
   "source": [
    "## 模型總結\n",
    "### 邏輯回歸 (Logistic Regression)：\n",
    "適用於二分類問題，通過使用邏輯函數（Sigmoid 函數）來估計概率，基於概率來決定分類。\n",
    "### 決策樹 (Decision Trees)：\n",
    "通過創建一棵決策樹來模擬決策過程，每個內部節點代表一個特徵上的判斷，每個分支代表一個判斷結果，而每個葉節點代表一個類別。\n",
    "### 隨機森林 (Random Forests)：\n",
    "為決策樹的集成學習方法，它建立多個決策樹來進行訓練和預測，通過投票方式來提高模型的準確性和穩定性。\n",
    "### 支持向量機 (Support Vector Machines, SVM)：\n",
    "通過找到最大邊際的超平面來區分不同的類別。SVM 在處理高維數據時表現良好，並且可以通過核技巧處理非線性數據。\n",
    "### K-近鄰算法 (K-Nearest Neighbors, KNN)：\n",
    "基於距離的算法，通過查找測試點最近的 K 個訓練點的類別來進行預測。KNN 算法簡單直觀，不需要訓練過程。\n",
    "### 貝氏分類器 (Naive Bayes)：\n",
    "基於貝葉斯定理，假設特徵之間彼此獨立。它在文本分類（如垃圾郵件過濾）中表現特別出色。\n",
    "### 神經網絡 (Neural Networks)：\n",
    "由大量的網絡層組成，包括輸入層、隱藏層和輸出層。深度學習是神經網絡的一種，適用於複雜的圖像和語音識別問題。\n",
    "\n",
    "- [statQuest neural networks](https://www.youtube.com/watch?v=CqOfi41LfDw&list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1&index=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0311f529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
